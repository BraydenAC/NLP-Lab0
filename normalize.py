#Lab 0: Text Normalizatoion
#Text selected: The Adventures of Huckleberry Finn
#Brayden Cloutier
#1/24/2025

import os
from enum import unique
import matplotlib.pyplot as plt
import nltk
import re
import numpy as np
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from collections import Counter

nltk.download('stopwords')

def main_menu(inputText, lowercasing, lemmatization, stopwordRemoval, cleanPunctuation):
    menuComplete = False
    while menuComplete == False:
        print("Type an option's corresponding letter to select it.")
        print("a) Choose New File")
        print("b) NormalizationSettings")
        print("c) Get Type List")
        print("d) Quit")

        # Get User Input
        userInput = input("Input:")
        if userInput == "a":
            inputText = select_file()
        elif userInput == "b":
            lowercasing, lemmatization, stopwordRemoval, cleanPunctuation = normalization_options(lowercasing, lemmatization, stopwordRemoval, cleanPunctuation)
        elif userInput == "c":
            output = preprocess_text(inputText, lowercasing, lemmatization, stopwordRemoval, cleanPunctuation)
            do_output(output)
        elif userInput == "d":
            menuComplete = True
        else:
            print("Invalid Input, please try again")

def do_output(preprocessedOutput):
    #Flattener and Counter generated by ChatGPT
    # Flatten the 2D list into a 1D list
    flatOutput = [token for row in preprocessedOutput for token in row]

    # Count occurrences of each token
    token_counts = Counter(flatOutput)

    # Get unique tokens and their counts
    unique_tokens = list(token_counts.keys())
    counts = list(token_counts.values())

    # Zip the arrays together and sort by the numeric array in decreasing order
    sorted_pairs = sorted(zip(counts, unique_tokens), key=lambda x: x[0], reverse=True)

    # Unzip the sorted pairs back into separate arrays
    counts, unique_tokens = zip(*sorted_pairs)

    menuComplete = False
    while menuComplete == False:
        # Ask the user what format they want the output to be in
        print("What format would you like to have your preprocessed text in?")
        print("a) Graph")
        print("b) Ranking List")
        print("c) Both")
        userInput = input("Input:")
        if userInput == "a":
            generate_graph(unique_tokens, counts)
            menuComplete = True
        elif userInput == "b":
            generate_list(unique_tokens, counts)
            menuComplete = True
        elif userInput == "c":
            generate_graph(unique_tokens, counts)
            generate_list(unique_tokens, counts)
            menuComplete = True
        else:
            print("Invalid Input, please try again")

#Generated partially by ChatGPT
def generate_graph(types, counts):
    # Generate ranks (1-based index)
    ranks = np.arange(1, len(counts) + 1)
    # Plot Zipfâ€™s Law graph (log-log scale)
    plt.figure(figsize=(8, 6))
    plt.loglog(ranks, counts, marker="o", linestyle="None", label="Word Frequencies")

    # Compute and plot the ideal Zipf's Law line
    f1 = counts[0]  # Frequency of the most common word
    ideal_zipf = f1 / ranks  # Zipf's ideal 1/rank distribution
    plt.loglog(ranks, ideal_zipf, linestyle="dashed", color="red", label="Zipf's Law (Ideal)")

    # Labels and title
    plt.xlabel("Rank")
    plt.ylabel("Frequency")
    plt.title("Zipf's Law: Word Frequency")
    plt.legend()
    plt.grid(True)

    # Show the plot
    plt.show()

def generate_list(types, counts):
    print(f"Total tokens: {sum(counts)}")
    for x in range(0, len(types), 1):
        print(f"{types[x]}: {counts[x]}")

def select_file():
    menuComplete = False
    while menuComplete == False:
        print("Please select one of the following files")

        # Specify the folder path
        folder_path = "Texts"
        # List all files in the folder (generated by chatGPT)
        files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]
        print(files)

        #get user input
        userInput = input("Type file name here: ")

        #check to see if input matches a file name
        try:
            with open("Texts/" + userInput, "r") as file:
                stringInput = file.read()
                menuComplete = True
        except:
            print("File not found, please try again")
        return stringInput


def normalization_options(lowercasing, lemmatization, stopwordRemoval, cleanPunctuation):
    menuComplete = False
    while menuComplete == False:
        #Print: "Type an option's corresponding letter to toggle it."
        print(f"a) Lowercasing: {lowercasing}")
        print(f"b) Lemmatization: {lemmatization}")
        print(f"c) Stopword Removal: {stopwordRemoval}")
        print(f"d) Remove Trailing Punctuation: {cleanPunctuation}")
        print(f"e) Back to Main Menu")

        #Get User Input
        userInput = input("Input: ")
        if userInput == "a":
            lowercasing = not lowercasing
        elif userInput == "b":
            lemmatization = not lemmatization
        elif userInput == "c":
            stopwordRemoval = not stopwordRemoval
        elif userInput == "d":
            cleanPunctuation = not cleanPunctuation
        elif userInput == "e":
            menuComplete = True
        else:
            print("Invalid Input, please try again")
    return lowercasing, lemmatization, stopwordRemoval, cleanPunctuation

def preprocess_text(inputString, lowercase, lemma, stopword, cleanPunctuation):
    newText = inputString
    if lowercase:
        #Do Lowercasing
        newText = newText.lower()

    #Tokenize the input
    x = 0
    tokenizedString = []
    for line in newText.splitlines():
        #Stores each line from input string as a tokenized array form of the original
        #TODO: Continue here!!!!!
        tokenizedString.append(line.split())
        x += 1
    newText = tokenizedString

    #Lemmatizer
    if lemma:
        #Lemmatize with NLTK
        lemmatizer = WordNetLemmatizer()
        newTexgt = [[lemmatizer.lemmatize(word) for word in row] for row in newText]

    #Stopword Removal
    if stopword:
        #Load external stopword list
        mystops = stopwords.words('english')
        #From class colab, apply stopwords and tokenize
        mystops = set(mystops)
        newText = [[tok for tok in row if tok not in mystops] for row in newText]

    if cleanPunctuation:
        #Generated by Chatgpt
        newText = [[re.sub(r'[^\w\s]', '', word) for word in row] for row in newText]

    #Row below generated by ChatGPT, removes empty string elements
    newText = [[element for element in row if element] for row in newText if row and any(row)]
    return newText

#Initialize variables
lowercasing = False
lemmatization = False
stopwordRemoval = False
cleanPunctuation = False

#Get first file name
inputText = select_file()
#Initialize menu
main_menu(inputText, lowercasing, lemmatization, stopwordRemoval, cleanPunctuation)